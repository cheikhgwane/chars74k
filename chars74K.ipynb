{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample directory : 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of one random image : (128, 128)\n",
      " Example image : \n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from random import randint\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "# path to directory\n",
    "base_dir = \"C:\\\\Users\\\\Cheikh\\\\Desktop\\\\projetChars74k\"\n",
    "\n",
    "fnt_base_dir = os.path.join(base_dir, \"EnglishFnt\")\n",
    "img_base_dir = os.path.join(base_dir, \"EnglishImg\")\n",
    "hnd_base_dir = os.path.join(base_dir, \"EnglishHnd\")\n",
    "\n",
    "# utils functions used to plot random image in dataset\n",
    "\n",
    "\n",
    "def showFolderImageSample(base_folder, img_number=2):\n",
    "    '''\n",
    "    base_folder(String) : directory in which we'll look for sample\n",
    "    img_number : for each folder how many img to show\n",
    "    '''\n",
    "    nrows = ncols = img_number\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(ncols * img_number, nrows * img_number)\n",
    "    dir_names = os.listdir(base_folder)\n",
    "\n",
    "    # get sample directory\n",
    "    sample_dir = []\n",
    "    for i in range(img_number):\n",
    "        sample_dir.append(dir_names[randint(0, 61)])\n",
    "    dir_names = [os.path.join(base_folder, dname) for dname in sample_dir]\n",
    "\n",
    "    print(\"total sample directory : {}\".format(len(dir_names)))\n",
    "    # for each sample directory get img_number random image\n",
    "\n",
    "    img = []\n",
    "    for d in dir_names:\n",
    "        for i in range(img_number):\n",
    "            img.append(os.path.join(d, os.listdir(\n",
    "                d)[randint(0, len(os.listdir(d)) - 1)]))\n",
    "\n",
    "    for i, img_path in enumerate(img):\n",
    "        # Set up subplot; subplot indices start at 1\n",
    "        sp = plt.subplot(nrows, ncols, i + 1)\n",
    "        sp.axis('Off')  # Don't show axes (or gridlines)\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "\n",
    "    plt.show()\n",
    "    print(\"Size of one random image : {}\".format(img.shape))\n",
    "    print(\" Example image : \\n {}\".format(img[0]))\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def dirTotalFile(base):\n",
    "    fileNumber = 0\n",
    "    if(os.path.isfile(base)):\n",
    "        return 0\n",
    "    _dir = os.listdir(base)\n",
    "    for d in _dir:\n",
    "        _d = os.path.join(base, d)\n",
    "        if(os.path.isdir(_d)):\n",
    "            fileNumber += dirTotalFile(_d)\n",
    "        else:\n",
    "            fileNumber += 1\n",
    "    return fileNumber\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "showFolderImageSample(fnt_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset image....\n",
      "100.00% \n",
      " Time for reading training images : 1.3115715980529785\n",
      "Reading test dataset image....\n",
      "100.00% \n",
      " Time for reading test images : 0.4641077518463135\n"
     ]
    }
   ],
   "source": [
    "_list = [i for i in range(0, 10)] + [chr(i) for i in range(97, 123)] + [chr(i) for i in range(65, 91)]\n",
    "\n",
    "\n",
    "class Utils:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.files = {}\n",
    "\n",
    "    def readImage(self, start, size, _type):\n",
    "        print('Reading {} dataset image....'.format(_type))\n",
    "        begin = time.time()\n",
    "        files = self.files\n",
    "        data = []\n",
    "        label = []\n",
    "        j = 0\n",
    "        #\n",
    "        for key in files.keys():\n",
    "            for i in range(start, start+size):\n",
    "                label.append(key)\n",
    "                data.append(plt.imread(files[key][i]))\n",
    "            j = j + 1.613\n",
    "            if(j > 100):\n",
    "                j = 100\n",
    "            sys.stdout.write(\"\\r{0:.2f}%\".format(j))\n",
    "            sys.stdout.flush()\n",
    "        end = time.time()\n",
    "        le = LabelEncoder()\n",
    "        label = le.fit_transform(label)\n",
    "        label = to_categorical(label)\n",
    "\n",
    "        print(\" \\n Time for reading {} images : {}\".format(_type, end-begin))\n",
    "        return np.array(data), label\n",
    "\n",
    "    def loadData(self):\n",
    "\n",
    "        train_images_size = 10 #775\n",
    "        test_size =  3 #241\n",
    "\n",
    "        path = self.path\n",
    "        files = self.files\n",
    "\n",
    "        if(not(os.path.exists(path))):\n",
    "            raise(\"Directory doesn't exist\")\n",
    "\n",
    "        dnames = [os.path.join(path, dname) for dname in os.listdir(path)]\n",
    "\n",
    "        # read all the image\n",
    "        for i, label in enumerate(_list):\n",
    "            if(not(os.path.isdir(dnames[i]))):\n",
    "                continue\n",
    "            files[label] = [os.path.join(dnames[i], f) for f in os.listdir(dnames[i])]\n",
    "        \n",
    "        train, train_label = self.readImage(0, train_images_size, \"training\")\n",
    "        \n",
    "        test, test_label = self.readImage(train_images_size, test_size, \"test\")\n",
    "        \n",
    "        return (train, train_label),(test, test_label)\n",
    "\n",
    "\n",
    "utils = Utils(fnt_base_dir)\n",
    "(train, train_label),(test, test_label) = utils.loadData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 128, 128, 32)      64        \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 524288)            0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 62)                32505918  \n",
      "=================================================================\n",
      "Total params: 32,505,982\n",
      "Trainable params: 32,505,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# keras font model\n",
    "\n",
    "print(train[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "nodes = 32\n",
    "optimizer = 'rmsprop'\n",
    "\n",
    "\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(32,activation = 'relu', input_shape = (128,128,1)))\n",
    "network.add(layers.Flatten())\n",
    "network.add(layers.Dense(62,activation='softmax'))\n",
    "network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_15_input to have 4 dimensions, but got array with shape (48050, 16384, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bb56b6ca90e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m history = network.fit(train, train_label, batch_size=32,\n\u001b[1;32m----> 7\u001b[1;33m                       epochs=epochs)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mhistory_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_15_input to have 4 dimensions, but got array with shape (48050, 16384, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "network.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['acc'])\n",
    "\n",
    "\n",
    "history = network.fit(train, train_label, batch_size=32,\n",
    "                      epochs=epochs)\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "acc_values = history_dict['acc']\n",
    "plt.rcParams['figure.figsize'] = (16, 8)  # Make the figures a bit bigger\n",
    "fig, (ax1, ax2,) = plt.subplots(1, 2)\n",
    "\n",
    "x = range(1, epochs+1)\n",
    "ax1.plot(x, loss_values, 'bo', label='Training Loss')\n",
    "ax1.set_title('Training loss ')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(x, acc_values, 'bo', label='Training Accuracy')\n",
    "ax2.set_title('Training accuracy ')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test,test_label)\n",
    "print('accuracy on test set : {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = network.predict_classes(test,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
